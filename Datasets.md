# Datasets

All great datascience requires some great datasets. Below are links to some of the more popular datasets that are often used as benchmarks for new algorithms.

## Imagenet
[Imagenet Home] (http://www.image-net.org/)



## CIFAR-10
The CIFAR-10 and CIFAR-100 are labeled subsets of the 80 million tiny images dataset. They were collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.

- **60,000 images of size 32 x 32**
- **10 Classes**
- **~6000 images per class**
- **50,000 training images**
- **10,000 test images**

<img src='https://snag.gy/4rlpZw.jpg' />
## CIFAR-100
<table>
<tr><td>Superclass</td><td>Classes</td></tr>
<tr><td>aquatic mammals</td><td>beaver, dolphin, otter, seal, whale</td></tr>
<tr><td>fish</td><td>aquarium fish, flatfish, ray, shark, trout</td></tr>
<tr><td>flowers</td><td>orchids, poppies, roses, sunflowers, tulips</td></tr>
<tr><td>food containers</td><td>bottles, bowls, cans, cups, plates</td></tr>
<tr><td>fruit and vegetables</td><td>apples, mushrooms, oranges, pears, sweet peppers</td></tr>
<tr><td>household electrical devices</td><td>clock, computer keyboard, lamp, telephone, </td></tr>television
<tr><td>household furniture</td><td>bed, chair, couch, table, wardrobe</td></tr>
<tr><td>insects</td><td>bee, beetle, butterfly, caterpillar, cockroach</td></tr>
<tr><td>large carnivores</td><td>bear, leopard, lion, tiger, wolf</td></tr>
<tr><td>large man-made outdoor things</td><td>bridge, castle, house, road, skyscraper</td></tr>
<tr><td>large natural outdoor scenes</td><td>cloud, forest, mountain, plain, sea</td></tr>
<tr><td>large omnivores and herbivores</td><td>camel, cattle, chimpanzee, elephant, </td></tr>kangaroo
<tr><td>medium-sized mammals</td><td>fox, porcupine, possum, raccoon, skunk</td></tr>
<tr><td>non-insect invertebrates</td><td>crab, lobster, snail, spider, worm</td></tr>
<tr><td>people</td><td>baby, boy, girl, man, woman</td></tr>
<tr><td>reptiles</td><td>crocodile, dinosaur, lizard, snake, turtle</td></tr>
<tr><td>small mammals</td><td>hamster, mouse, rabbit, shrew, squirrel</td></tr>
<tr><td>trees</td><td>maple, oak, palm, pine, willow</td></tr>
<tr><td>vehicles 1</td><td>bicycle, bus, motorcycle, pickup truck, train</td></tr>
<tr><td>vehicles 2</td><td>lawn-mower, rocket, streetcar, tank, tractor</td></tr>
</table>

## MNIST

[Link to homepage](http://yann.lecun.com/exdb/mnist/)

The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.

It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting.

The original black and white (bilevel) images from NIST were size normalized to fit in a 20x20 pixel box while preserving their aspect ratio. The resulting images contain grey levels as a result of the anti-aliasing technique used by the normalization algorithm. the images were centered in a 28x28 image by computing the center of mass of the pixels, and translating the image so as to position this point at the center of the 28x28 field.

<img src='http://i.ytimg.com/vi/0QI3xgXuB-Q/hqdefault.jpg'/>

## SVHN - Street View House Numbers

['Link to Homepage'](http://ufldl.stanford.edu/housenumbers/)

SVHN is a real-world image dataset for developing machine learning and object recognition algorithms with minimal requirement on data preprocessing and formatting. It can be seen as similar in flavor to MNIST (e.g., the images are of small cropped digits), but incorporates an order of magnitude more labeled data (over 600,000 digit images) and comes from a significantly harder, unsolved, real world problem (recognizing digits and numbers in natural scene images). SVHN is obtained from house numbers in Google Street View images. 


<img src="http://ufldl.stanford.edu/housenumbers/examples_new.png" />


## Speech Commands Dataset

['Link to hosting page (google)'](https://research.googleblog.com/2017/08/launching-speech-commands-dataset.html)

To solve these problems, the TensorFlow and AIY teams have created the Speech Commands Dataset, and used it to add training* and inference sample code to TensorFlow. The dataset has 65,000 one-second long utterances of 30 short words, by thousands of different people, contributed by members of the public through the AIY website. It’s released under a Creative Commons BY 4.0 license, and will continue to grow in future releases as more contributions are received. The dataset is designed to let you build basic but useful voice interfaces for applications, with common words like “Yes”, “No”, digits, and directions included. The infrastructure we used to create the data has been open sourced too, and we hope to see it used by the wider community to create their own versions, especially to cover underserved languages and applications.


<img src="https://3.bp.blogspot.com/-Mo-sJK2QM5U/WZ31e7iMvHI/AAAAAAAAB-A/IEzEr4P6up4jagW2H0NcxyK3w26DQ_kVgCLcBGAs/s400/image1.png" />

## Audioset - Labeled Audio Clips

[hosted by google](https://research.googleblog.com/2017/03/announcing-audioset-dataset-for-audio.html)

In order to address this, we recently released AudioSet, a collection of over 2 million ten-second YouTube excerpts labeled with a vocabulary of 527 sound event categories, with at least 100 examples for each category. Announced in our paper at the IEEE International Conference on Acoustics, Speech, and Signal Processing, AudioSet provides a common, realistic-scale evaluation task for audio event detection and a starting point for a comprehensive vocabulary of sound events, designed to advance research into audio event detection and recognition. 

<img src="https://1.bp.blogspot.com/-V1eBvymMYK8/WN00mlaVHoI/AAAAAAAABqM/sNvS811qyBA2YHg2c0_W4Qfa3WkOHsW_ACLcB/s640/image01.png" />

<img src="https://4.bp.blogspot.com/-aFpmH4QCAvY/WN00tJ_O6-I/AAAAAAAABqQ/foXm-2kBjpkbc55-ui6yx_-N6EgYuq86ACLcB/s640/image03.png" />

<img src="https://1.bp.blogspot.com/-OZv7NYeee3M/WN00_ZBnRUI/AAAAAAAABqU/yCIVntSaHJYpFWIIJjDkaDcSokGKB_DBwCLcB/s1600/image02.png" />

## Youtube 8M Labeled Video Dataset

[Youtube labeled videos](https://research.google.com/youtube8m/)

YouTube-8M is a large-scale labeled video dataset that consists of millions of YouTube video IDs and associated labels from a diverse vocabulary of 4700+ visual entities. It comes with precomputed state-of-the-art audio-visual features from billions of frames and audio segments, designed to fit on a single hard disk. This makes it possible to get started on this dataset by training a baseline video model in less than a day on a single machine! At the same time, the dataset's scale and diversity can enable deep exploration of complex audio-visual models that can take weeks to train even in a distributed fashion. 

<img src="https://snag.gy/dwODPh.jpg" />


## Youtube Bounding Box Dataset

[Hosted by Google](https://research.googleblog.com/2017/02/advancing-research-on-video.html)

Today, in order to facilitate progress in video understanding research, we are introducing YouTube-BoundingBoxes, a dataset consisting of 5 million bounding boxes spanning 23 object categories, densely labeling segments from 210,000 YouTube videos. To date, this is the largest manually annotated video dataset containing bounding boxes, which track objects in temporally contiguous frames. The dataset is designed to be large enough to train large-scale models, and be representative of videos captured in natural settings. Importantly, the human-labelled annotations contain objects as they appear in the real world with partial occlusions, motion blur and natural lighting.

<img src="https://3.bp.blogspot.com/-nsvuVAp9KpY/WJi1EPDDgiI/AAAAAAAABhM/IBqiFsiqrdQFTFvVrduA4spnAGBGwrqIgCLcB/s640/image00.png" />


